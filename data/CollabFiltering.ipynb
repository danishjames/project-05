{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "from surprise import SVD, NormalPredictor\n",
    "from surprise import AlgoBase\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import cross_validate, train_test_split, GridSearchCV\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo = MongoClient()\n",
    "\n",
    "db = mongo['project-05']\n",
    "\n",
    "db_user = db['Usernames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listens_df = pd.DataFrame(columns=['Username', 'Track ID', 'Playcount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "cursor = db_user.find(batch_size=50, limit=500, no_cursor_timeout=True)\n",
    "\n",
    "for document in cursor:\n",
    "    if document['Tracks'] is None or len(document['Tracks']) == 0:\n",
    "        print(document)\n",
    "        db_user.delete_one(document)\n",
    "    else:\n",
    "        # Playcounts used as ratings, weighted by the most played track (highest value possible is 1)\n",
    "        max_plays = int(document['Tracks'][0][2])\n",
    "        for i in range(len(document['Tracks'])):\n",
    "            string = (document['Tracks'][i][0] + '_' + document['Tracks'][i][1]).replace(' ', '_')\n",
    "            trackid = re.sub('[^A-Za-z0-9_]', '', string=string)\n",
    "            username = document['Username']\n",
    "            weight = np.log(int(document['Tracks'][i][2]) / max_plays) + 10\n",
    "            series = pd.Series([username, trackid, weight], index=listens_df.columns)\n",
    "            listens_df = listens_df.append(series, ignore_index=True)\n",
    "    counter += 1\n",
    "    \n",
    "    print(counter)\n",
    "    \n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listens_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.load_from_df(listens_df[[\"Username\", \"Track ID\", \"Playcount\"]], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dumb Model\n",
    "\n",
    "With every good model comes a dumb model for comparison. In order to see just how good our future models perform, we will set up a dumb algorithm that predicts the average rating for all items in the database. Thus, the predicted rating will be the same no matter what we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DumbAlgo(AlgoBase):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Always call base method before doing anything.\n",
    "        AlgoBase.__init__(self)\n",
    "\n",
    "    def fit(self, trainset):\n",
    "\n",
    "        # Need to call base method first again\n",
    "        AlgoBase.fit(self, trainset)\n",
    "\n",
    "        # Compute the average rating. This assumes ratings come in the (userid, itemid, rating) format\n",
    "        self.the_mean = np.mean([r for (_, _, r) in self.trainset.all_ratings()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "\n",
    "        return self.the_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_algo = DumbAlgo()\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "dumb_algo.fit(trainset)\n",
    "dumb_predictions = dumb_algo.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.rmse(dumb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_cv = cross_validate(dumb_algo, data, measures=['RMSE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Predictor\n",
    "\n",
    "Another dumb model, this time assuming the ratings fall along a normal distribution and making predictions based on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_algo = NormalPredictor()\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "norm_algo.fit(trainset)\n",
    "norm_predictions = norm_algo.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.rmse(norm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(norm_algo, data, measures=['RMSE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD Algorithm\n",
    "\n",
    "Below we shall check our parameters and make recommendations using the SVD algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_epochs\": [10, 15, 20],\n",
    "    \"lr_all\": [0.0025, 0.005, 0.0075],\n",
    "    \"reg_all\": [0.2, 0.5, 0.8]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(SVD, param_grid, measures=[\"rmse\", \"mae\"], cv=5, verbose=True)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "print(gs.best_score[\"rmse\"])\n",
    "print(gs.best_params[\"rmse\"])\n",
    "\n",
    "print(gs.best_score[\"mae\"])\n",
    "print(gs.best_params[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = SVD(n_epochs=20, lr_all=0.0025, reg_all=0.2, verbose=True)\n",
    "\n",
    "# let's do train-test-split, where test set is 25% of the ratings\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_cv = cross_validate(algo, data, measures=['RMSE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Dummy', 'SVD']\n",
    "\n",
    "dumb_array = dumb_cv['test_rmse']\n",
    "svd_array = svd_cv['test_rmse']\n",
    "\n",
    "dumb_mean = np.mean(dumb_array)\n",
    "svd_mean = np.mean(svd_array)\n",
    "\n",
    "rmse = [dumb_mean, svd_mean]\n",
    "\n",
    "dumb_std = np.std(dumb_array)*3\n",
    "svd_std = np.std(svd_array)*3\n",
    "\n",
    "error = [dumb_std, svd_std]\n",
    "\n",
    "x_pos = np.arange(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.bar(x_pos, rmse, align='center', alpha=0.8, color='#657e93', capsize=15)\n",
    "ax.set_ylabel('Root Mean Square Error (RMSE)', fontsize=14)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(models, fontsize=14)\n",
    "ax.set_title('Model Error Comparison', fontsize=18)\n",
    "\n",
    "# Save the figure and show\n",
    "plt.tight_layout()\n",
    "plt.savefig('bar_plot_with_error_bars.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random user selected\n",
    "\n",
    "listens_df[listens_df['Username'] == 'aaron250401'].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to make predictions, we will create an anti-set which only contains user/item pairings without existing ratings\n",
    "\n",
    "pred_algo = SVD(n_epochs=20, lr_all=0.0025, reg_all=0.2, verbose=True)\n",
    "\n",
    "pred_trainset = data.build_full_trainset()\n",
    "\n",
    "pred_algo.fit(pred_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_testset = pred_trainset.build_anti_testset()\n",
    "predictions_test = pred_algo.test(pred_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(user, predictions, n=10):\n",
    "    '''Return the top-N recommendation for a user from a set of predictions.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_n('aaron250401', predictions, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
